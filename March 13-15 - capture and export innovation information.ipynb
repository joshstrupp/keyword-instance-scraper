{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4391ee0cce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.joshstrupp.com/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://www.joshstrupp.com/\"\n",
    "print (soup.get_text(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d71ee63493df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.joshstrupp.com/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "url = \"https://www.joshstrupp.com/\"\n",
    "print (soup.get_text(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://www.joshstrupp.com/\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "text = soup.get_text()\n",
    "print (text.count('Product' or 'Fake' or 'Josh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-08c761accdb9>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-65-08c761accdb9>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print 'Found the word \"{0}\" {1} times'\\n.format(searched_word, len(results))\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import re\n",
    "\n",
    "data = 'https://www.amerisourcebergen.com/about-who-we-are'\n",
    "\n",
    "# ''''\n",
    "# <html>\n",
    "# <body>\n",
    "# <div>today is a sunny day</div>\n",
    "# <div>I love when it's sunny outside</div>\n",
    "# Call me sunny\n",
    "# <div>sunny is a cool word sunny</div>\n",
    "# </body>\n",
    "# </html>\n",
    "# ''''\n",
    "\n",
    "searched_word = 'innovate'\n",
    "\n",
    "soup = bs4.BeautifulSoup(data, 'html.parser')\n",
    "results = soup.body.find_all(string=re.compile('.*{0}.*'.format(searched_word)), recursive=True)\n",
    "\n",
    "print 'Found the word \"{0}\" {1} times\\n'.format(searched_word, len(results))\n",
    "\n",
    "for content in results:\n",
    "    words = content.split()\n",
    "    for index, word in enumerate(words):\n",
    "        # If the content contains the search word twice or more this will fire for each occurence\n",
    "        if word == searched_word:\n",
    "            print 'Whole content: \"{0}\"'.format(content)\n",
    "            before = None\n",
    "            after = None\n",
    "            # Check if it's a first word\n",
    "            if index != 0:\n",
    "                before = words[index-1]\n",
    "            # Check if it's a last word\n",
    "            if index != len(words)-1:\n",
    "                after = words[index+1]\n",
    "            print '\\tWord before: \"{0}\", word after: \"{1}\"'.format(before, after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Powered by our associates around the world, we provide the pharmaceutical products and business solutions that improve access to care. We operate the backbone of the healthcare supply chain. We drive the future of local care delivery. We guide medical innovations to market. This is what it means to create healthier futures.', \"Helping people access the healthcare products they need is in our DNA. Whether through product sourcing and distribution, supporting community-based care or partnering with manufacturers to bring an innovative product to market, our commitment to improving patients' lives makes all the difference.\"]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://www.amerisourcebergen.com/about-who-we-are\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "innovation_instances = soup.find_all(string=re.compile(\"inno*\"))\n",
    "print(innovation_instances)\n",
    "\n",
    "# text = soup.get_text()\n",
    "# print (text.count('Product' or 'Fake' or 'Josh'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "\n",
    "url = \"https://www.amerisourcebergen.com/about-who-we-are\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "innovation_instances = soup.find_all(string=re.compile(\"inno*\"))\n",
    "# print(innovation_instances)\n",
    "\n",
    "def createCSV():\n",
    "    with open(\"innovation_sentences.csv\", mode=\"w\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([\"Amerisourcebergen\"])\n",
    "        csvwriter.writerow([innovation_instances])\n",
    "        \n",
    "createCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "url = \"https://www.amerisourcebergen.com/about-who-we-are\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "innovation_instances = soup.find_all(string=re.compile(\"inno*\"))\n",
    "# print(innovation_instances)\n",
    "\n",
    "data = {'AmeriSource':[{'all_instances': innovation_instances}]}\n",
    "\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# define two URLs for two different companies\n",
    "url = \"https://www.amerisourcebergen.com/about-who-we-are\"\n",
    "url2 = \"https://www.aboutamazon.com/our-company\"\n",
    "\n",
    "html = urlopen(url)\n",
    "html2 = urlopen(url2)\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "\n",
    "innovation_instances_AmeriSource = soup.find_all(string=re.compile(\"inno*\"))\n",
    "innovation_instances_Amazon = soup2.find_all(string=re.compile(\"inno*\"))\n",
    "\n",
    "data = {'AmeriSource':[{'all_instances': innovation_instances_AmeriSource}]},{'Amazon':[{'all_instances': innovation_instances_Amazon}]}\n",
    "\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# define two URLs for two different companies\n",
    "url = \"https://www.amerisourcebergen.com/about-who-we-are\"\n",
    "url2 = \"https://www.aboutamazon.com/our-company\"\n",
    "\n",
    "html = urlopen(url)\n",
    "html2 = urlopen(url2)\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "\n",
    "innovation_instances_AmeriSource = soup.find_all(string=re.compile(\"inno*\"))\n",
    "innovation_instances_Amazon = soup2.find_all(string=re.compile(\"inno*\"))\n",
    "\n",
    "data = {'AmeriSource':[{'all_instances': innovation_instances_AmeriSource}]},{'Amazon':[{'all_instances': innovation_instances_Amazon}]}\n",
    "\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://www.aboutamazon.com/our-company': ['From the lab to the fulfillment center, employees contributed to the innovations behind a mailer that can be recycled in the same bin as Amazon’s iconic box.', 'Our innovations'], 'https://www.amerisourcebergen.com/about-who-we-are': ['Powered by our associates around the world, we provide the pharmaceutical products and business solutions that improve access to care. We operate the backbone of the healthcare supply chain. We drive the future of local care delivery. We guide medical innovations to market. This is what it means to create healthier futures.', \"Helping people access the healthcare products they need is in our DNA. Whether through product sourcing and distribution, supporting community-based care or partnering with manufacturers to bring an innovative product to market, our commitment to improving patients' lives makes all the difference.\"]}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "urls = ['https://www.aboutamazon.com/our-company', 'https://www.amerisourcebergen.com/about-who-we-are']\n",
    "data = {}\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    innovation_instances = soup.find_all(string=re.compile(\"inno*\"))\n",
    "    \n",
    "    data[url] = innovation_instances\n",
    "\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "    \n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "\n",
    "urls = ['https://www.aboutamazon.com/our-company', 'https://www.amerisourcebergen.com/about-who-we-are']\n",
    "data = {}\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    innovation_instances = soup.find_all(string=re.compile(\"inno*\"))\n",
    "    \n",
    "    data[url] = innovation_instances\n",
    "\n",
    "def createCSV():\n",
    "    with open(\"data.csv\", mode=\"w\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([data])\n",
    "        \n",
    "createCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "{'https://www.aboutamazon.com/our-company': ['From the lab to the fulfillment center, employees contributed to the innovations behind a mailer that can be recycled in the same bin as Amazon’s iconic box.', 'Our innovations'], 'https://www.amerisourcebergen.com/about-who-we-are': ['Powered by our associates around the world, we provide the pharmaceutical products and business solutions that improve access to care. We operate the backbone of the healthcare supply chain. We drive the future of local care delivery. We guide medical innovations to market. This is what it means to create healthier futures.', \"Helping people access the healthcare products they need is in our DNA. Whether through product sourcing and distribution, supporting community-based care or partnering with manufacturers to bring an innovative product to market, our commitment to improving patients' lives makes all the difference.\"]}\n"
     ]
    }
   ],
   "source": [
    "#GOAL: product count of \"innovation\" words for each website\n",
    "#NOTE: there are simple JSON to CSV converters: https://json-csv.com\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "import requests\n",
    "from urllib import request, response, error, parse\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "#Use \"String slices\" and .title() method to entract name of company after 'http://www.'\n",
    "\n",
    "urls = ['https://www.aboutamazon.com/our-company', 'https://www.amerisourcebergen.com/about-who-we-are']\n",
    "data = {}\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    innovation_instances = soup.find_all(string=re.compile(\"inno*\"))\n",
    "    \n",
    "    # add count of innovation words to dictionary and print as JSON/CSV element under \n",
    "    \n",
    "    data[url] = innovation_instances\n",
    "\n",
    "\n",
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "    \n",
    "    \n",
    "print (data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
