from bs4 import BeautifulSoup
import urllib.request
import re
from IPython.display import HTML
import requests
from urllib import request, response, error, parse
from urllib.request import Request, urlopen
import json
import csv
import time
from datetime import datetime 
import os, ssl
# import nltk
# nltk.download()


# Original test URLs

# urls = [
#         'http://www.intel.com/content/www/us/en/company-overview/company-overview.html',
#         'http://www.corporate.ford.com/company.html',
#         'http://www.boeing.com/company/general-info',
#         'http://www.alliancedata.com/about-us/default.aspx',
#         'https://www.polarityte.com/about/overview/',
#         'http://www.arconic.com/what-we-do/',
#         'http://www.fico.com/en/about-us/']


# URLs with no return
# 'http://www.aboutamazon.com',
# 'http://abc.xyz/',
# 'http://about.fb.com/',
# 'http://Pfizer.com'
# 'http://Microsoft.com',
# 'http://www.jnj.com',
# 'http://www.cisco.com/c/en/us/about.html',
# 'http://www.ibm.com/ibm/us/en/']





#must remove s from https


# #Apple and  works but returns tons of HTML

# Working URLs

urls = [

# 'http://Intel.com',
# 'http://Apple.com',
# 'http://Merck.com',
# 'http://corporate.ford.com',
# 'http://www.gm.com',
# 'http://Oracle.com',
# 'http://Celgene.com',
'http://QUALCOMM.com']
# 'http://www.lilly.com',
# 'http://AbbVie.com',
# 'http://Bristol-Myers.com',
# 'http://www.ge.com',
# 'http://Gilead.com',
# 'http://Amgen.com',
# 'http://Broadcom.com',
# 'http://www.boeing.com']

data = {}


# function that scrapes each URL 
def scrape(url):
    
    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    ssl._create_default_https_context = ssl._create_unverified_context
    
    html = urlopen(req, context=ssl.SSLContext()).read()
    soup = BeautifulSoup(html, 'html.parser')
        
    innovation_instances = soup.find_all(string=re.compile("inno+"))
    
#     for instance in innovation_instances:
#         if instance.endswith('-' or '/') or instance.startswith('-' or '/'):
#             innovation_instances.remove(instance)
        

    def listToString():  
        ii_string = " " 

        # return string   
        return (ii_string.join(innovation_instances)) 

    inno_strings = listToString()
    sub = 'inno'
    
    inno_string_count = (inno_strings[0:1000].count(sub))
    total_count = (inno_strings.count(sub))
    
    
    data[url] = inno_strings[0:1000], inno_string_count, total_count   
    
    print(data)        

    
#     with open("Qualcomm_truncate_test.json", 'w') as outfile:
#             json.dump(data, outfile)

    
for url in urls:
      
#     time.sleep(3)
    
    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    ssl._create_default_https_context = ssl._create_unverified_context

    html = urlopen(req, context=ssl.SSLContext()).read()                   
    soup = BeautifulSoup(html, 'html.parser')
    

    
        # expand search
    links = [a['href'] for a in soup.find_all('a', href=True)] 

    base_url = url.split('/')
    base_url_joined = ''.join(base_url[0]) + '//' + base_url[2]



    for link in links: 
        if link[0:1] == '/' and link.endswith('.pdf' or '.svg') == False and "^privacy" or "^terms-of-service" not in link:
            scrape(base_url_joined + link)





